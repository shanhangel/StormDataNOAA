web1[1][[4]]
web1[[1]][1]
web1[[1]][2]
web1[[1]][3]
web1[[1]][4]
?combine
??combine
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]], stringsAsFactors=FALSE)
web <- c(readLines(url[i], web))
}
web
web <- c(readLines(url[2], web))
web
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]], stringsAsFactors=FALSE)
web <- combine(readLines(url[i], web))
}
web
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]], stringsAsFactors=FALSE)
web <- append(readLines(url[i], web))
}
web <- readLines(url[c(1:6)])
web <- readLines(url[1:6])
web[1] <- readLines(url[1:6])
web[1] <- readLines(url[1])
for (i in 2:6){
web[i] <- readLines(url[i])
}
library(XML)
url <- paste("http://www.premierleague.com/en-gb/players/ea-sports-player-performance-index.html?paramSearchTerm=&paramClubId=&paramSeason=2013-2014&paramPosition=&paramEaBreakdownType=ACCUMULATIVE&paramGameWeek=1&paramItemsPerPage=100&paramSelectedPageIndex=", 1:6, sep="")
tbl <- readHTMLTable(url[1])[[1]]
web_1 <- readLines(url[1])
web_2 <- readLines(url[2])
web_3 <- readLines(url[3])
web_4 <- readLines(url[4])
web_5 <- readLines(url[5])
web_6 <- readLines(url[6])
web <- web[grep("/en-gb/players/profile.overview.html/", web)]
web1 <- strsplit(web, '"')
playerurl_1 <- web1[[1]]
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]], stringsAsFactors=FALSE)
}
for (i in 1:100){
web_1 <- web_1[grep("/en-gb/players/profile.overview.html/", web_1)]
web_1 <- strsplit(web, '"')
playerurl_1 <- web1[[1]]
}
for (i in 1:100){
web_1 <- web_1[grep("/en-gb/players/profile.overview.html/", web_1)]
web_1 <- strsplit(web, '"')
playerurl_1 <- web_1[[i]][5]
}
for (i in 1:100){
web_1 <- web_1[grep("/en-gb/players/profile.overview.html/", web_1)]
web_1 <- strsplit(web, '"')
playerurl_1[i] <- web_1[[i]][4]
}
web_1
web1[[1]]
web_1 <- readLines(url[1])
web_1 <- web_1[grep("/en-gb/players/profile.overview.html/", web_1)]
web_1 <- strsplit(web, '"')
web1[[1]]
web_1[1]
web_1
library(XML)
url <- paste("http://www.premierleague.com/en-gb/players/ea-sports-player-performance-index.html?paramSearchTerm=&paramClubId=&paramSeason=2013-2014&paramPosition=&paramEaBreakdownType=ACCUMULATIVE&paramGameWeek=1&paramItemsPerPage=100&paramSelectedPageIndex=", 1:6, sep="")
tbl <- readHTMLTable(url[1])[[1]]
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]], stringsAsFactors=FALSE)
}
web_1 <- readLines(url[1])
playerurl_1 <- vector()
web_1 <- web_1[grep("/en-gb/players/profile.overview.html/", web_1)]
web_1 <- strsplit(web_1, '"')
for (i in 1:100){
playerurl_1[i] <- web_1[[i]][4]
}
web_2 <- readLines(url[2])
playerurl_2 <- vector()
web_2 <- web_2[grep("/en-gb/players/profile.overview.html/", web_2)]
web_2 <- strsplit(web_2, '"')
for (i in 1:100){
playerurl_2[i] <- web_2[[i]][4]
}
web_3 <- readLines(url[3])
playerurl_3 <- vector()
web_3 <- web_3[grep("/en-gb/players/profile.overview.html/", web_3)]
web_3 <- strsplit(web_3, '"')
for (i in 1:100){
playerurl_3[i] <- web_3[[i]][4]
}
web_4 <- readLines(url[4])
playerurl_4 <- vector()
web_4 <- web_4[grep("/en-gb/players/profile.overview.html/", web_4)]
web_4 <- strsplit(web_4, '"')
for (i in 1:100){
playerurl_4[i] <- web_4[[i]][4]
}
web_5 <- readLines(url[5])
playerurl_5 <- vector()
web_5 <- web_5[grep("/en-gb/players/profile.overview.html/", web_5)]
web_5 <- strsplit(web_5, '"')
for (i in 1:100){
playerurl_5[i] <- web_5[[i]][4]
}
web_6 <- readLines(url[6])
playerurl_6 <- vector()
web_6 <- web_6[grep("/en-gb/players/profile.overview.html/", web_6)]
web_6 <- strsplit(web_6, '"')
for (i in 1:length(web_6)){
playerurl_6[i] <- web_6[[i]][4]
}
wholeplayer <- c(playerurl_1, playerurl_2, playerurl_3, playerurl_4,
playerurl_5, playerurl_6)
playerurl <- paste("http://www.premierleague.com", wholeplayer, sep="")
Age <- vector()
for (i in 1:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
colnames(tbl)[2] <- "Age"
write.csv(tbl, "playerppi.csv")
Age1 <- Age
for (i in length(Age):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- Age
playerurl[1]
Age2 <- Age[228:531]
Age <- c(Age1, Age2)
Age <- unlist(c(Age1, Age2))
colnames(tbl)[2] <- "Age"
View(tbl)
colnames(tbl)[2] <- "Age"
Age <- c(Age1, Age2)
Age <- unlist(Age)
Age
Age <- c(Age1, Age2)
Age
Age[Age==NULL] <- NA
Age <- unlist(Age)
Age <- c(Age1, Age2)
Age[Age==NULL] <- "NA"
Age
is.na(Age)
is.null(Age)
is.NULL(Age)
?is.null
?unlist
which(Age==27)
which(Age=="27")
which(Age==NULL)
which(Age=="NULL")
Age[Age="NULL"] <- "NA"
Age
unlist(Age)
index <- which(Age=="NULL")
Age[index] <- 0
Age
Age <- c(Age1, Age2)
Age[Age="NULL"] <- "NA"
Age
Age <- c(Age1, Age2)
Age[Age="NULL"] <- 0
Age <- unlist(Age)
Age
Age <- c(Age1, Age2)
index <- which(Age=="NULL")
Age[index] <- "NA"
Age
Age <- unlist(Age)
colnames(tbl)[2] <- "Age"
View(tbl)
tbl <- readHTMLTable(url[1])[[1]]
tbl <- readHTMLTable(url[1], stringsAsFactors=FALSE)[[1]]
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]], stringsAsFactors=FALSE)
}
View(tbl)
View(tbl)
tbl <- readHTMLTable(url[1])[[1]]
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]])
}
View(tbl)
colnames(tbl)[2] <- "Age"
View(tbl)
tbl[,2] <- Age
View(tbl)
playerurl[40]
playerurl[531]
write.csv(tbl, "playerppi.csv")
getwd
getwd()
library(XML)
url <- paste("http://www.premierleague.com/en-gb/players/ea-sports-player-performance-index.html?paramSearchTerm=&paramClubId=&paramSeason=2013-2014&paramPosition=&paramEaBreakdownType=ACCUMULATIVE&paramGameWeek=1&paramItemsPerPage=100&paramSelectedPageIndex=", 1:6, sep="")
tbl <- readHTMLTable(url[1])[[1]]
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]])
}
## debug了几遍，发现问题出在两个方面：1.有些player的名字是非英文，如"Yaya Touré"对应
## 的url后缀为“yaya-toure”，所以要得到url不能用名字组合的方式，而应该从表格页面抓取。
## 用readLines函数可以读取那几页表格，找到对应的包含url的行数(特征是都含有“/en-gb/players/profile.overview.html/”）,
## 再以""符号分隔，得到类似"en-gb/players/profile.overview.html/luis-suarez"的元素，
## 问题1就解决了。
web_1 <- readLines(url[1])
playerurl_1 <- vector()
web_1 <- web_1[grep("/en-gb/players/profile.overview.html/", web_1)]
web_1 <- strsplit(web_1, '"')
for (i in 1:100){
playerurl_1[i] <- web_1[[i]][4]
}
web_2 <- readLines(url[2])
playerurl_2 <- vector()
web_2 <- web_2[grep("/en-gb/players/profile.overview.html/", web_2)]
web_2 <- strsplit(web_2, '"')
for (i in 1:100){
playerurl_2[i] <- web_2[[i]][4]
}
web_3 <- readLines(url[3])
playerurl_3 <- vector()
web_3 <- web_3[grep("/en-gb/players/profile.overview.html/", web_3)]
web_3 <- strsplit(web_3, '"')
for (i in 1:100){
playerurl_3[i] <- web_3[[i]][4]
}
web_4 <- readLines(url[4])
playerurl_4 <- vector()
web_4 <- web_4[grep("/en-gb/players/profile.overview.html/", web_4)]
web_4 <- strsplit(web_4, '"')
for (i in 1:100){
playerurl_4[i] <- web_4[[i]][4]
}
web_5 <- readLines(url[5])
playerurl_5 <- vector()
web_5 <- web_5[grep("/en-gb/players/profile.overview.html/", web_5)]
web_5 <- strsplit(web_5, '"')
for (i in 1:100){
playerurl_5[i] <- web_5[[i]][4]
}
web_6 <- readLines(url[6])
playerurl_6 <- vector()
web_6 <- web_6[grep("/en-gb/players/profile.overview.html/", web_6)]
web_6 <- strsplit(web_6, '"')
for (i in 1:length(web_6)){
playerurl_6[i] <- web_6[[i]][4]
}
## 以上几个重复代码可以用循环简化
wholeplayer <- c(playerurl_1, playerurl_2, playerurl_3, playerurl_4,
playerurl_5, playerurl_6)
playerurl <- paste("http://www.premierleague.com", wholeplayer, sep="")
## 有了整个playerurl以后用xpathSApply解析Age的值，共有531个页面
Age <- vector()
for (i in 1:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age1 <- Age
## 问题2，有了531个playerurl以后，要访问531个页面，看起来网站并没有设限制，但是依然
## 在访问两百多页以后出现错误。可以采用两个解析html函数，第二个从第一
## 个函数结束的那个元素开始运行，再将两次的结果合并，就可以解决问题了
j <- length(Age)
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- Age[(j+1):531]
Age <- c(Age1, Age2)
index <- which(Age=="NULL")
Age[index] <- "NA"
Age <- unlist(Age)
tbl[,2] <- Age
colnames(tbl)[2] <- "Age"
write.csv(tbl, "playerppi.csv")
Age
Age <- vector()
for (i in 1:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age1 <- Age
## 问题2，有了531个playerurl以后，要访问531个页面，看起来网站并没有设限制，但是依然
## 在访问两百多页以后出现错误。可以采用两个解析html函数，第二个从第一
## 个函数结束的那个元素开始运行，再将两次的结果合并，就可以解决问题了
j <- length(Age)
if (j<531){
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- Age[(j+1):531]
Age <- c(Age1, Age2)
}
else {
Age <- Age1
}
j <- length(Age)
if (j<531){
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- Age[(j+1):531]
Age <- c(Age1, Age2)
}
if else {
Age <- Age1
}
j <- length(Age)
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- Age[(j+1):531]
Age <- c(Age1, Age2)
Age
Age1 <- vector()
Age1 <- Age
j <- length(Age)
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- vector()
Age2 <- Age[(j+1):531]
Age <- c(Age1, Age2)
rm(list=ls(all=TRUE
))
library(XML)
url <- paste("http://www.premierleague.com/en-gb/players/ea-sports-player-performance-index.html?paramSearchTerm=&paramClubId=&paramSeason=2013-2014&paramPosition=&paramEaBreakdownType=ACCUMULATIVE&paramGameWeek=1&paramItemsPerPage=100&paramSelectedPageIndex=", 1:6, sep="")
tbl <- readHTMLTable(url[1])[[1]]
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]])
}
## debug了几遍，发现问题出在两个方面：1.有些player的名字是非英文，如"Yaya Touré"对应
## 的url后缀为“yaya-toure”，所以要得到url不能用名字组合的方式，而应该从表格页面抓取。
## 用readLines函数可以读取那几页表格，找到对应的包含url的行数(特征是都含有“/en-gb/players/profile.overview.html/”）,
## 再以""符号分隔，得到类似"en-gb/players/profile.overview.html/luis-suarez"的元素，
## 问题1就解决了。
web_1 <- readLines(url[1])
playerurl_1 <- vector()
web_1 <- web_1[grep("/en-gb/players/profile.overview.html/", web_1)]
web_1 <- strsplit(web_1, '"')
for (i in 1:100){
playerurl_1[i] <- web_1[[i]][4]
}
web_2 <- readLines(url[2])
playerurl_2 <- vector()
web_2 <- web_2[grep("/en-gb/players/profile.overview.html/", web_2)]
web_2 <- strsplit(web_2, '"')
for (i in 1:100){
playerurl_2[i] <- web_2[[i]][4]
}
web_3 <- readLines(url[3])
playerurl_3 <- vector()
web_3 <- web_3[grep("/en-gb/players/profile.overview.html/", web_3)]
web_3 <- strsplit(web_3, '"')
for (i in 1:100){
playerurl_3[i] <- web_3[[i]][4]
}
web_4 <- readLines(url[4])
playerurl_4 <- vector()
web_4 <- web_4[grep("/en-gb/players/profile.overview.html/", web_4)]
web_4 <- strsplit(web_4, '"')
for (i in 1:100){
playerurl_4[i] <- web_4[[i]][4]
}
web_5 <- readLines(url[5])
playerurl_5 <- vector()
web_5 <- web_5[grep("/en-gb/players/profile.overview.html/", web_5)]
web_5 <- strsplit(web_5, '"')
for (i in 1:100){
playerurl_5[i] <- web_5[[i]][4]
}
web_6 <- readLines(url[6])
playerurl_6 <- vector()
web_6 <- web_6[grep("/en-gb/players/profile.overview.html/", web_6)]
web_6 <- strsplit(web_6, '"')
for (i in 1:length(web_6)){
playerurl_6[i] <- web_6[[i]][4]
}
## 以上几个重复代码可以用循环简化
wholeplayer <- c(playerurl_1, playerurl_2, playerurl_3, playerurl_4,
playerurl_5, playerurl_6)
playerurl <- paste("http://www.premierleague.com", wholeplayer, sep="")
## 有了整个playerurl以后用xpathSApply解析Age的值，共有531个页面
Age <- vector()
for (i in 1:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age1 <- vector()
Age1 <- Age
## 问题2，有了531个playerurl以后，要访问531个页面，看起来网站并没有设限制，但是依然
## 在访问两百多页以后出现错误。可以采用两个解析html函数，第二个从第一
## 个函数结束的那个元素开始运行，再将两次的结果合并，就可以解决问题了
j <- length(Age)
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- vector()
Age2 <- Age[(j+1):531]
Age <- c(Age1, Age2)
index <- which(Age=="NULL")
Age[index] <- "NA"
Age <- unlist(Age)
tbl[,2] <- Age
colnames(tbl)[2] <- "Age"
write.csv(tbl, "playerppi.csv")
j <- length(Age)
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
j <- length(Age)
for (i in (j+1):531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age <- vector()
Age <- vector()
for (i in 1:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age1 <- vector()
Age1 <- Age
## 问题2，有了531个playerurl以后，要访问531个页面，看起来网站并没有设限制，但是依然
## 在访问两百多页以后出现错误。可以采用两个解析html函数，第二个从第一
## 个函数结束的那个元素开始运行，再将两次的结果合并，就可以解决问题了
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- vector()
Age2 <- Age[228:531]
Age <- c(Age1, Age2)
j <- length(Age)
for (i in 228:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
}
Age <- vector()
for (i in 1:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age1 <- vector()
Age1 <- Age
## 问题2，有了531个playerurl以后，要访问531个页面，看起来网站并没有设限制，但是依然
## 在访问两百多页以后出现错误。可以采用两个解析html函数，第二个从第一
## 个函数结束的那个元素开始运行，再将两次的结果合并，就可以解决问题了
Age2 <- vector()
for (i in 228:531){
Age[i] <- xpathSApply(htmlTreeParse(playerurl[i], useInternal=TRUE),
"//td[@class='normal']", xmlValue)[3]
}
Age2 <- Age[228:531]
Age <- c(Age1, Age2)
Age
Age <- Age[1:531]
index <- which(Age=="NULL")
Age[index] <- "NA"
Age <- unlist(Age)
Age
tbl[,2] <- Age
colnames(tbl)[2] <- "Age"
write.csv(tbl, "playerppi.csv")
View(tbl)
class(url)
web_1
unlist(web_1)
a <- unlist(web_1)
a
a[1]
a[3]
a[4]
index_1 <- seq(4:498, 4)
index_1 <- seq(4,498, 4)
a[index_1]
a
index_1 <- seq(4,498,5)
a[index_1]
install.packages(c("ggplot2", "mvtnorm", "party", "Rcpp", "rjson", "tm", "wordcloud"))
demo()
image
image()
time
Time
quit
quit()
w
library(XML)
url <- paste("http://www.premierleague.com/en-gb/players/ea-sports-player-performance-index.html?paramSearchTerm=&paramClubId=&paramSeason=2013-2014&paramPosition=&paramEaBreakdownType=ACCUMULATIVE&paramGameWeek=1&paramItemsPerPage=100&paramSelectedPageIndex=", 1:6, sep="")
tbl <- readHTMLTable(url[1])[[1]]
for (i in 2:6){
tbl <- rbind(tbl, readHTMLTable(url[i])[[1]])
}
web <- c(web1, web2, web3, web4, web5, web6)
web_1 <- readLines(url[1])
web_1
library(knitr)
knit2html("stormdata.Rmd")
setwd("./StormDataNOAA")
knit2html("stormdata.Rmd")
knit2html("stormdata.Rmd")
knit2html("stormdata.Rmd")
knit2html("stormdata.Rmd")
knit2html("stormdata.Rmd")
